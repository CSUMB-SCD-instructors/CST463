{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Explainability Demo\n",
    "\n",
    "This notebook demonstrates three key techniques for understanding what Convolutional Neural Networks learn:\n",
    "\n",
    "1. **Filter Visualizations**: See what patterns individual filters detect\n",
    "2. **Activation Maximization**: Generate images that maximally activate specific filters\n",
    "3. **Class Activation Maps (Grad-CAM)**: Visualize which input regions drive predictions\n",
    "\n",
    "We'll use a simple 2-layer CNN trained on either MNIST or Fashion-MNIST to keep things interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Choose your dataset and training parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:13:09.021820Z",
     "start_time": "2025-10-29T14:13:09.018217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: MNIST dataset\n",
      "Model path: saved_models/mnist_simple_cnn.h5\n"
     ]
    }
   ],
   "source": [
    "# Dataset selection - change this to 'fashion_mnist' to switch datasets\n",
    "DATASET = 'mnist'  # Options: 'mnist' or 'fashion_mnist'\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_NEW_MODEL = False  # Set to True to train from scratch, False to load pre-trained\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Model will be saved/loaded from:\n",
    "MODEL_PATH = f'saved_models/{DATASET}_simple_cnn.h5'\n",
    "\n",
    "print(f\"Configuration: {DATASET.upper()} dataset\")\n",
    "print(f\"Model path: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:13:12.202026Z",
     "start_time": "2025-10-29T14:13:09.032422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check TensorFlow version and GPU availability\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Class labels for visualization\n",
    "if DATASET == 'mnist':\n",
    "    CLASS_NAMES = [str(i) for i in range(10)]\n",
    "else:  # fashion_mnist\n",
    "    CLASS_NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:13:13.873584Z",
     "start_time": "2025-10-29T14:13:13.611377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mnist: 60000 training samples, 10000 test samples\n",
      "Image shape: (28, 28, 1)\n",
      "Value range: [-1.00, 1.00]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "if DATASET == 'mnist':\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "else:  # fashion_mnist\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# Normalize to [-1, 1] range\n",
    "x_train = (x_train - 0.5) * 2\n",
    "x_test = (x_test - 0.5) * 2\n",
    "\n",
    "print(f\"Loaded {DATASET}: {len(x_train)} training samples, {len(x_test)} test samples\")\n",
    "print(f\"Image shape: {x_train.shape[1:]}\")\n",
    "print(f\"Value range: [{x_train.min():.2f}, {x_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Simple CNN Architecture\n",
    "\n",
    "We use a deliberately simple architecture:\n",
    "- **Conv Layer 1**: 8 filters, 3x3 kernel\n",
    "- **Conv Layer 2**: 8 filters, 3x3 kernel\n",
    "- **Fully Connected**: Output layer\n",
    "\n",
    "This small architecture makes it easier to visualize and understand what each filter learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:13:36.301060Z",
     "start_time": "2025-10-29T14:13:36.261666Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssogden/repos/classes/CST463-golden/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,970</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │           \u001b[38;5;34m292\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m4\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,970\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,342</span> (9.15 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,342\u001b[0m (9.15 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,342</span> (9.15 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,342\u001b[0m (9.15 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_simple_cnn():\n",
    "    \"\"\"Create a simple 2-layer CNN for visualization.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First convolutional layer: 8 filters, 3x3 kernel\n",
    "        layers.Conv2D(8, (3, 3), activation='relu', padding='same', \n",
    "                     input_shape=(28, 28, 1), name='conv1'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool1'),  # 28x28 -> 14x14\n",
    "        \n",
    "        # Second convolutional layer: 8 filters, 3x3 kernel\n",
    "        layers.Conv2D(4, (3, 3), activation='relu', padding='same', name='conv2'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool2'),  # 14x14 -> 7x7\n",
    "        \n",
    "        # Fully connected layer\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_simple_cnn()\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (or Load Pre-trained Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:13:51.239703Z",
     "start_time": "2025-10-29T14:13:39.037593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model...\n",
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8093 - loss: 0.6424 - val_accuracy: 0.9238 - val_loss: 0.2499\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9389 - loss: 0.2036 - val_accuracy: 0.9532 - val_loss: 0.1508\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9576 - loss: 0.1406 - val_accuracy: 0.9644 - val_loss: 0.1116\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9651 - loss: 0.1142 - val_accuracy: 0.9695 - val_loss: 0.0934\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9695 - loss: 0.0997 - val_accuracy: 0.9720 - val_loss: 0.0830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final test accuracy: 97.20%\n",
      "Model saved to saved_models/mnist_simple_cnn.h5\n"
     ]
    }
   ],
   "source": [
    "# Train or load model\n",
    "if TRAIN_NEW_MODEL:\n",
    "    print(\"Training new model...\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(x_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate final accuracy\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"\\nFinal test accuracy: {test_acc*100:.2f}%\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    os.makedirs('saved_models', exist_ok=True)\n",
    "    model.save(MODEL_PATH)\n",
    "    print(f\"Model saved to {MODEL_PATH}\")\n",
    "    \n",
    "else:\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(f\"Loading pre-trained model from {MODEL_PATH}\")\n",
    "        model = keras.models.load_model(MODEL_PATH)\n",
    "        \n",
    "        # Evaluate to show performance\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print(f\"Model loaded successfully!\")\n",
    "        print(f\"Test accuracy: {test_acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"No pre-trained model found at {MODEL_PATH}\")\n",
    "        print(\"Set TRAIN_NEW_MODEL = True to train a new model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Method 1: Filter Visualizations\n",
    "\n",
    "We'll visualize what patterns the convolutional filters learn in two ways:\n",
    "1. **Filter weights**: The raw learned kernels\n",
    "2. **Feature maps**: What the filters activate on when shown real images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Visualize Filter Weights (Raw Values + Heatmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:13:59.398839Z",
     "start_time": "2025-10-29T14:13:58.954766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONV1 - Filter Weights\n",
      "============================================================\n",
      "Shape: (3, 3, 1, 8) (height, width, in_channels, filters)\n",
      "\n",
      "Raw weight values for Filter 0, Channel 0:\n",
      "[[-0.37896422 -0.49241692 -0.11159258]\n",
      " [ 0.26522166 -0.25866368  0.03942073]\n",
      " [ 0.5167847   0.40683505  0.05655925]]\n",
      "\n",
      "Min: -0.4924, Max: 0.5168\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x150 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONV2 - Filter Weights\n",
      "============================================================\n",
      "Shape: (3, 3, 8, 4) (height, width, in_channels, filters)\n",
      "\n",
      "Raw weight values for Filter 0, Channel 0:\n",
      "[[-0.06146873 -0.04542069 -0.5543591 ]\n",
      " [ 0.4688225   0.43856588 -0.08236688]\n",
      " [ 0.26103443  0.13101475 -0.04104816]]\n",
      "\n",
      "Min: -0.5544, Max: 0.4688\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x1200 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_conv_filters(model, layer_name):\n",
    "    \"\"\"\n",
    "    Visualize the learned filters of a convolutional layer.\n",
    "    Shows both raw float values and heatmap visualizations.\n",
    "    \"\"\"\n",
    "    # Get the layer\n",
    "    layer = model.get_layer(layer_name)\n",
    "    \n",
    "    # Get filter weights: shape is (height, width, in_channels, out_channels)\n",
    "    weights = layer.get_weights()[0]  # [0] is weights, [1] is biases\n",
    "    num_filters = weights.shape[3]\n",
    "    in_channels = weights.shape[2]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{layer_name.upper()} - Filter Weights\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Shape: {weights.shape} (height, width, in_channels, filters)\\n\")\n",
    "    \n",
    "    # Print raw values for first filter as example\n",
    "    print(f\"Raw weight values for Filter 0, Channel 0:\")\n",
    "    print(weights[:, :, 0, 0])\n",
    "    print(f\"\\nMin: {weights[:, :, 0, 0].min():.4f}, Max: {weights[:, :, 0, 0].max():.4f}\\n\")\n",
    "    \n",
    "    # Visualize all filters\n",
    "    fig, axes = plt.subplots(in_channels, num_filters, figsize=(num_filters * 1.5, in_channels * 1.5))\n",
    "    if in_channels == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(in_channels):\n",
    "        for j in range(num_filters):\n",
    "            ax = axes[i, j] if in_channels > 1 else axes[0, j]\n",
    "            \n",
    "            # Get filter weights for this channel and filter\n",
    "            w = weights[:, :, i, j]\n",
    "            \n",
    "            # Normalize for visualization\n",
    "            im = ax.imshow(w, cmap='RdBu', vmin=-abs(w).max(), vmax=abs(w).max())\n",
    "            \n",
    "            if i == 0:\n",
    "                ax.set_title(f'Filter {j}', fontsize=10)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f'In Ch {i}', fontsize=10)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{layer_name.upper()} - Learned Filters (red=positive, blue=negative)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize both convolutional layers\n",
    "visualize_conv_filters(model, 'conv1')\n",
    "visualize_conv_filters(model, 'conv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Visualize Feature Maps (Filter Activations on Real Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T14:14:03.281560Z",
     "start_time": "2025-10-29T14:14:03.220004Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_feature_maps(model, image, label, class_names):\n",
    "    \"\"\"\n",
    "    Visualize what each filter \"sees\" when processing a real image.\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    image_batch = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    predictions = model.predict(image_batch, verbose=0)\n",
    "    prediction = np.argmax(predictions[0])\n",
    "    \n",
    "    # Create an explicit input tensor\n",
    "    input_tensor = keras.Input(shape=(28, 28, 1))\n",
    "    \n",
    "    # Create models that output intermediate layer activations using explicit input\n",
    "    conv1_output = model.get_layer('conv1')(input_tensor)\n",
    "    conv1_model = keras.Model(inputs=input_tensor, outputs=conv1_output)\n",
    "    \n",
    "    # For conv2, we need to pass through pool1 first\n",
    "    pool1_output = model.get_layer('pool1')(conv1_output)\n",
    "    conv2_output = model.get_layer('conv2')(pool1_output)\n",
    "    conv2_model = keras.Model(inputs=input_tensor, outputs=conv2_output)\n",
    "    \n",
    "    # Get activations\n",
    "    conv1_acts = conv1_model.predict(image_batch, verbose=0)[0]  # Shape: (H, W, num_filters)\n",
    "    conv2_acts = conv2_model.predict(image_batch, verbose=0)[0]  # Shape: (H, W, num_filters)\n",
    "    \n",
    "    # Get filter weights and number of filters\n",
    "    conv1_weights = model.get_layer('conv1').get_weights()[0]  # Shape: (3, 3, 1, num_filters)\n",
    "    conv2_weights = model.get_layer('conv2').get_weights()[0]  # Shape: (3, 3, in_channels, num_filters)\n",
    "    \n",
    "    num_conv1_filters = conv1_weights.shape[3]\n",
    "    num_conv2_filters = conv2_weights.shape[3]\n",
    "    \n",
    "    # Display original image\n",
    "    input_img = image[:, :, 0]\n",
    "    \n",
    "    print(f\"\\nTrue Label: {class_names[label]} | Prediction: {class_names[prediction]}\")\n",
    "    \n",
    "    # Plot Conv1: Filters + Activations\n",
    "    fig, axes = plt.subplots(3, num_conv1_filters, figsize=(num_conv1_filters * 2, 6))\n",
    "    \n",
    "    # Handle case where there's only one filter\n",
    "    if num_conv1_filters == 1:\n",
    "        axes = axes.reshape(3, 1)\n",
    "    \n",
    "    # Row 1: Original image in first cell, rest empty\n",
    "    axes[0, 0].imshow(input_img, cmap='gray')\n",
    "    axes[0, 0].set_title('Input', fontsize=9)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    for i in range(1, num_conv1_filters):\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Row 2: All filter weights\n",
    "    for i in range(num_conv1_filters):\n",
    "        filter_weights = conv1_weights[:, :, 0, i]\n",
    "        axes[1, i].imshow(filter_weights, cmap='RdBu', vmin=-abs(filter_weights).max(), vmax=abs(filter_weights).max())\n",
    "        axes[1, i].set_title(f'Filter {i}', fontsize=9)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    # Row 3: All activations\n",
    "    for i in range(num_conv1_filters):\n",
    "        axes[2, i].imshow(conv1_acts[:, :, i], cmap='viridis')\n",
    "        axes[2, i].set_title(f'Act {i}', fontsize=9)\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Conv1: {num_conv1_filters} Filters - Weights (middle) → Activations (bottom)', fontsize=12, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Conv2: Show filter weights for first input channel + activations\n",
    "    fig, axes = plt.subplots(2, num_conv2_filters, figsize=(num_conv2_filters * 2, 4))\n",
    "    \n",
    "    # Handle case where there's only one filter\n",
    "    if num_conv2_filters == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    # Row 1: Show filter weights (first input channel only for simplicity)\n",
    "    for i in range(num_conv2_filters):\n",
    "        filter_weights = conv2_weights[:, :, 0, i]  # Just show first input channel\n",
    "        axes[0, i].imshow(filter_weights, cmap='RdBu', vmin=-abs(filter_weights).max(), vmax=abs(filter_weights).max())\n",
    "        axes[0, i].set_title(f'Filter {i}\\n(ch 0)', fontsize=8)\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Row 2: Activations\n",
    "    for i in range(num_conv2_filters):\n",
    "        axes[1, i].imshow(conv2_acts[:, :, i], cmap='viridis')\n",
    "        axes[1, i].set_title(f'Act {i}', fontsize=9)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Conv2: {num_conv2_filters} Filters - Weights (top, 1st input channel) → Activations (bottom)', fontsize=12, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some activation statistics\n",
    "    print(\"\\nActivation statistics for Conv1 Filter 0:\")\n",
    "    print(f\"Shape: {conv1_acts[:, :, 0].shape}\")\n",
    "    print(f\"Min: {conv1_acts[:, :, 0].min():.4f}, Max: {conv1_acts[:, :, 0].max():.4f}\")\n",
    "    print(f\"Mean: {conv1_acts[:, :, 0].mean():.4f}, Std: {conv1_acts[:, :, 0].std():.4f}\")\n",
    "\n",
    "# Get a sample image from test set\n",
    "sample_idx = 0  # Change this to see different examples\n",
    "sample_image = x_test[sample_idx]\n",
    "sample_label = y_test[sample_idx]\n",
    "\n",
    "visualize_feature_maps(model, sample_image, sample_label, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different sample images to see how filters respond differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiple examples\n",
    "for idx in [0, 10, 100, 500]:  # Different samples\n",
    "    sample_image = x_test[idx]\n",
    "    sample_label = y_test[idx]\n",
    "    visualize_feature_maps(model, sample_image, sample_label, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Method 2: Activation Maximization\n",
    "\n",
    "Instead of showing what filters respond to on real images, we can **generate** images that maximally activate specific filters. This helps us understand what pattern the filter is \"looking for.\"\n",
    "\n",
    "We start with random noise and use gradient ascent to modify the input to maximize the activation of a chosen filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_maximization(model, layer_name, filter_idx, iterations=100, lr=1.0):\n",
    "    \"\"\"\n",
    "    Generate an input image that maximally activates a specific filter.\n",
    "    \n",
    "    Args:\n",
    "        model: The CNN model\n",
    "        layer_name: Which layer to maximize ('conv1' or 'conv2')\n",
    "        filter_idx: Which filter in that layer to maximize\n",
    "        iterations: Number of optimization steps\n",
    "        lr: Learning rate for gradient ascent\n",
    "    \"\"\"\n",
    "    # Get the filter weights to display\n",
    "    layer = model.get_layer(layer_name)\n",
    "    filter_weights = layer.get_weights()[0]  # Shape: (3, 3, in_channels, num_filters)\n",
    "    \n",
    "    # Create an explicit input tensor and build the model up to target layer\n",
    "    input_tensor = keras.Input(shape=(28, 28, 1))\n",
    "    \n",
    "    # Build layers up to the target layer\n",
    "    x = model.get_layer('conv1')(input_tensor)\n",
    "    if layer_name == 'conv1':\n",
    "        layer_output = x\n",
    "        # For conv1, show the single input channel\n",
    "        target_filter = filter_weights[:, :, 0, filter_idx]\n",
    "    else:\n",
    "        x = model.get_layer('pool1')(x)\n",
    "        x = model.get_layer('conv2')(x)\n",
    "        if layer_name == 'conv2':\n",
    "            layer_output = x\n",
    "            # For conv2, show all input channels\n",
    "            target_filter = filter_weights[:, :, :, filter_idx]\n",
    "        else:\n",
    "            # Continue building if needed for other layers\n",
    "            layer_output = x\n",
    "            target_filter = None\n",
    "    \n",
    "    feature_extractor = keras.Model(inputs=input_tensor, outputs=layer_output)\n",
    "    \n",
    "    # Start with random noise (scaled to match expected input range)\n",
    "    input_img = tf.Variable(tf.random.normal((1, 28, 28, 1)) * 0.1)\n",
    "    \n",
    "    # Track loss over iterations\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            activation = feature_extractor(input_img)\n",
    "            \n",
    "            # Get the activation of the target filter\n",
    "            # Shape: (1, height, width, num_filters)\n",
    "            filter_activation = activation[:, :, :, filter_idx]\n",
    "            \n",
    "            # We want to MAXIMIZE the mean activation\n",
    "            loss = -tf.reduce_mean(filter_activation)\n",
    "        \n",
    "        losses.append(-loss.numpy())  # Store positive value for plotting\n",
    "        \n",
    "        # Get gradients\n",
    "        grads = tape.gradient(loss, input_img)\n",
    "        \n",
    "        # Normalize gradients to prevent explosion\n",
    "        grads = grads / (tf.math.reduce_std(grads) + 1e-8)\n",
    "        \n",
    "        # Gradient ascent step\n",
    "        input_img.assign_add(lr * grads)\n",
    "        \n",
    "        # Clip values to reasonable range (instead of aggressive normalization)\n",
    "        input_img.assign(tf.clip_by_value(input_img, -2, 2))\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Iteration {i+1}/{iterations}, Activation: {losses[-1]:.4f}\")\n",
    "    \n",
    "    # Visualize result\n",
    "    result_img = input_img[0, :, :, 0].numpy()\n",
    "    \n",
    "    # Create figure based on layer type\n",
    "    if layer_name == 'conv1':\n",
    "        # Simple 3-panel layout for conv1\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        # Show filter weights\n",
    "        axes[0].imshow(target_filter, cmap='RdBu', vmin=-abs(target_filter).max(), vmax=abs(target_filter).max())\n",
    "        axes[0].set_title(f'{layer_name.upper()} Filter {filter_idx}\\nWeights (3x3)')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Show generated image\n",
    "        axes[1].imshow(result_img, cmap='gray', vmin=-2, vmax=2)\n",
    "        axes[1].set_title(f'Generated Image\\nMaximizes Activation')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Show optimization progress\n",
    "        axes[2].plot(losses)\n",
    "        axes[2].set_xlabel('Iteration')\n",
    "        axes[2].set_ylabel('Mean Activation')\n",
    "        axes[2].set_title('Optimization Progress')\n",
    "        axes[2].grid(True)\n",
    "        \n",
    "    else:\n",
    "        # For conv2, create a more complex layout with filter grid\n",
    "        num_channels = filter_weights.shape[2]  # Number of input channels\n",
    "        rows = 2\n",
    "        cols = int(np.ceil(num_channels / 2))\n",
    "        \n",
    "        # Create figure with custom layout: filter grid | generated image | progress plot\n",
    "        fig = plt.figure(figsize=(16, 5))\n",
    "        gs = fig.add_gridspec(2, 3, width_ratios=[1.5, 1, 1.5], hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Left: Grid of filter weights for all input channels\n",
    "        ax_filters = fig.add_subplot(gs[:, 0])\n",
    "        ax_filters.axis('off')\n",
    "        \n",
    "        # Create subplots for each channel\n",
    "        inner_gs = gs[0:2, 0].subgridspec(rows, cols, hspace=0.2, wspace=0.2)\n",
    "        for ch in range(num_channels):\n",
    "            row = ch // cols\n",
    "            col = ch % cols\n",
    "            ax = fig.add_subplot(inner_gs[row, col])\n",
    "            filter_ch = filter_weights[:, :, ch, filter_idx]\n",
    "            ax.imshow(filter_ch, cmap='RdBu', vmin=-abs(filter_ch).max(), vmax=abs(filter_ch).max())\n",
    "            ax.set_title(f'Ch {ch}', fontsize=9)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Add title for filter section\n",
    "        ax_filters.set_title(f'{layer_name.upper()} Filter {filter_idx}\\n({num_channels} input channels)', \n",
    "                            fontsize=11, pad=10)\n",
    "        \n",
    "        # Middle: Generated image\n",
    "        ax_img = fig.add_subplot(gs[:, 1])\n",
    "        ax_img.imshow(result_img, cmap='gray', vmin=-2, vmax=2)\n",
    "        ax_img.set_title('Generated Image\\nMaximizes Activation', fontsize=11)\n",
    "        ax_img.axis('off')\n",
    "        \n",
    "        # Right: Optimization progress\n",
    "        ax_progress = fig.add_subplot(gs[:, 2])\n",
    "        ax_progress.plot(losses)\n",
    "        ax_progress.set_xlabel('Iteration')\n",
    "        ax_progress.set_ylabel('Mean Activation')\n",
    "        ax_progress.set_title('Optimization Progress', fontsize=11)\n",
    "        ax_progress.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal activation: {losses[-1]:.4f}\")\n",
    "    print(f\"Image stats - Min: {result_img.min():.4f}, Max: {result_img.max():.4f}\")\n",
    "    \n",
    "    return result_img\n",
    "\n",
    "# Generate images that maximize different filters\n",
    "print(\"\\nGenerating images for Conv1 filters:\")\n",
    "for filter_idx in range(4):  # Show first 4 filters\n",
    "    print(f\"\\n--- Conv1 Filter {filter_idx} ---\")\n",
    "    activation_maximization(model, 'conv1', filter_idx, iterations=100, lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Conv2 filters (more abstract patterns)\n",
    "print(\"\\nGenerating images for Conv2 filters:\")\n",
    "for filter_idx in range(4):  # Show first 4 filters\n",
    "    print(f\"\\n--- Conv2 Filter {filter_idx} ---\")\n",
    "    activation_maximization(model, 'conv2', filter_idx, iterations=100, lr=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Method 3: Class Activation Maps (Grad-CAM)\n",
    "\n",
    "Grad-CAM shows which parts of the input image are most important for the network's prediction. It works by:\n",
    "1. Computing gradients of the predicted class with respect to feature maps\n",
    "2. Weighting feature maps by these gradients\n",
    "3. Creating a heatmap showing important regions\n",
    "\n",
    "This helps answer: \"What is the network looking at when it makes this prediction?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(model, image, target_class=None):\n",
    "    \"\"\"\n",
    "    Generate a Grad-CAM heatmap showing which image regions influence the prediction.\n",
    "    \n",
    "    Args:\n",
    "        model: The CNN model\n",
    "        image: Input image (28, 28, 1)\n",
    "        target_class: Class to generate CAM for (None = predicted class)\n",
    "    \"\"\"\n",
    "    # Prepare image\n",
    "    img_array = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    # Create an explicit input tensor and build model\n",
    "    input_tensor = keras.Input(shape=(28, 28, 1))\n",
    "    \n",
    "    # Build the network manually through all layers\n",
    "    x = model.get_layer('conv1')(input_tensor)\n",
    "    x = model.get_layer('pool1')(x)\n",
    "    conv2_output = model.get_layer('conv2')(x)\n",
    "    x = model.get_layer('pool2')(conv2_output)\n",
    "    \n",
    "    # Flatten layer - get the actual name from the model\n",
    "    flatten_layers = [layer for layer in model.layers if 'flatten' in layer.name.lower()]\n",
    "    if flatten_layers:\n",
    "        x = flatten_layers[0](x)\n",
    "    else:\n",
    "        x = layers.Flatten()(x)\n",
    "    \n",
    "    predictions = model.get_layer('output')(x)\n",
    "    \n",
    "    # Create model that outputs both conv2 and final predictions\n",
    "    grad_model = keras.Model(inputs=input_tensor, outputs=[conv2_output, predictions])\n",
    "    \n",
    "    # Get predictions\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        \n",
    "        # Get predicted class if not specified\n",
    "        if target_class is None:\n",
    "            target_class = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        # Get the score for the target class\n",
    "        class_channel = predictions[:, target_class]\n",
    "    \n",
    "    # Compute gradients of the class score with respect to conv2 output\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    \n",
    "    # Global average pooling of gradients (importance weights)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))  # Shape: (8,)\n",
    "    \n",
    "    print(f\"\\nGrad-CAM weights for target class '{CLASS_NAMES[target_class]}':\")\n",
    "    print(f\"Shape: {pooled_grads.shape} (one weight per filter)\")\n",
    "    print(f\"Weights: {pooled_grads.numpy()}\")\n",
    "    print(f\"\\nInterpretation: Positive weights mean that filter's activation increases prediction confidence\")\n",
    "    \n",
    "    # Weighted combination of feature maps\n",
    "    conv_outputs = conv_outputs[0]  # Remove batch dimension\n",
    "    \n",
    "    # Multiply each filter by its weight and sum\n",
    "    cam = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    \n",
    "    # Apply ReLU (only positive influences)\n",
    "    cam = tf.nn.relu(cam)\n",
    "    \n",
    "    # Normalize\n",
    "    cam = cam - tf.reduce_min(cam)\n",
    "    cam = cam / (tf.reduce_max(cam) + 1e-8)\n",
    "    \n",
    "    # Resize to original image size\n",
    "    cam = tf.image.resize(cam[..., tf.newaxis], (28, 28))\n",
    "    cam = cam.numpy()[:, :, 0]\n",
    "    \n",
    "    return cam, target_class\n",
    "\n",
    "\n",
    "def visualize_grad_cam(model, image, label, class_names):\n",
    "    \"\"\"\n",
    "    Visualize Grad-CAM heatmap overlaid on original image.\n",
    "    \"\"\"\n",
    "    cam, predicted_class = grad_cam(model, image)\n",
    "    \n",
    "    # Get original image\n",
    "    orig_img = image[:, :, 0]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(orig_img, cmap='gray')\n",
    "    axes[0].set_title(f'Original Image\\nTrue: {class_names[label]}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Heatmap\n",
    "    im = axes[1].imshow(cam, cmap='jet')\n",
    "    axes[1].set_title(f'Grad-CAM Heatmap\\nPredicted: {class_names[predicted_class]}')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(orig_img, cmap='gray')\n",
    "    axes[2].imshow(cam, cmap='jet', alpha=0.5)  # Overlay with transparency\n",
    "    axes[2].set_title('Overlay\\n(red = important for prediction)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nCAM statistics:\")\n",
    "    print(f\"Min: {cam.min():.4f}, Max: {cam.max():.4f}\")\n",
    "    print(f\"Mean: {cam.mean():.4f}, Std: {cam.std():.4f}\")\n",
    "\n",
    "# Visualize Grad-CAM for several examples\n",
    "print(\"\\nGenerating Grad-CAM visualizations:\")\n",
    "for idx in [0, 10, 100, 500]:\n",
    "    sample_image = x_test[idx]\n",
    "    sample_label = y_test[idx]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sample {idx}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    visualize_grad_cam(model, sample_image, sample_label, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Grad-CAM for Different Classes\n",
    "\n",
    "We can also generate Grad-CAM for a specific class (not necessarily the predicted one) to see what the network would look for if trying to classify the image as that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_class_cams(model, image, label, class_names, classes_to_compare=None):\n",
    "    \"\"\"\n",
    "    Compare Grad-CAM for different target classes on the same image.\n",
    "    \"\"\"\n",
    "    if classes_to_compare is None:\n",
    "        classes_to_compare = [label, (label + 1) % 10, (label + 2) % 10]\n",
    "    \n",
    "    orig_img = image[:, :, 0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(classes_to_compare) + 1, figsize=(4 * (len(classes_to_compare) + 1), 4))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(orig_img, cmap='gray')\n",
    "    axes[0].set_title(f'Original\\nTrue: {class_names[label]}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Generate CAM for each class\n",
    "    for i, target_class in enumerate(classes_to_compare):\n",
    "        cam, _ = grad_cam(model, image, target_class=target_class)\n",
    "        \n",
    "        axes[i + 1].imshow(orig_img, cmap='gray')\n",
    "        axes[i + 1].imshow(cam, cmap='jet', alpha=0.5)\n",
    "        axes[i + 1].set_title(f'CAM for\\n\"{class_names[target_class]}\"')\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Grad-CAM for Different Target Classes\\n(Shows what network looks for to classify as each class)', \n",
    "                 fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: see what features matter for different class predictions\n",
    "sample_image = x_test[0]\n",
    "sample_label = y_test[0]\n",
    "compare_class_cams(model, sample_image, sample_label, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Next Steps\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "1. **Filter Visualizations**: Convolutional filters learn edge detectors, texture patterns, and shape components\n",
    "   - Early layers (Conv1) detect simple patterns (edges, corners)\n",
    "   - Later layers (Conv2) detect more complex combinations\n",
    "\n",
    "2. **Activation Maximization**: We can synthesize images that maximally activate specific filters\n",
    "   - Shows what each filter is \"looking for\"\n",
    "   - Often reveals interesting patterns that aren't obvious from weights alone\n",
    "\n",
    "3. **Grad-CAM**: Highlights which image regions drive predictions\n",
    "   - Shows the model is (hopefully) looking at relevant features\n",
    "   - Can help debug incorrect predictions\n",
    "   - Different target classes focus on different regions\n",
    "\n",
    "## Experiments to Try\n",
    "\n",
    "1. **Switch datasets**: Change `DATASET = 'fashion_mnist'` and re-run to see how filters differ\n",
    "2. **Modify architecture**: Add more filters or layers and see how complexity affects learned features\n",
    "3. **Tune activation maximization**: Adjust learning rate and iterations for clearer generated images\n",
    "4. **Find failure cases**: Use Grad-CAM to understand misclassifications\n",
    "\n",
    "## Questions to Consider\n",
    "\n",
    "- Do Conv1 filters look similar across MNIST and Fashion-MNIST?\n",
    "- What happens to Grad-CAM when the model makes a wrong prediction?\n",
    "- Can you identify which filters are most important for specific digit/class recognition?\n",
    "- How do activation maximization results differ between Conv1 and Conv2 filters?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
