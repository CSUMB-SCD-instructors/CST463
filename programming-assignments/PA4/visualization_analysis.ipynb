{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA4: CNN Training Dynamics - Analysis & Visualization\n",
    "\n",
    "**Name:** [Your Name Here]\n",
    "\n",
    "**Date:** [Date]\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores CNN architectures, training dynamics, hyperparameter tuning, and overfitting through systematic experimentation. All visualizations use **matplotlib.pyplot only** (no seaborn).\n",
    "\n",
    "## Learning Objectives\n",
    "- Compare sequential vs. functional CNN architectures\n",
    "- Evaluate different optimizers and their convergence behavior\n",
    "- Design and conduct systematic hyperparameter search\n",
    "- Identify, induce, and mitigate overfitting\n",
    "- Assess effectiveness of training callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import student implementations\n",
    "from student_code import (\n",
    "    build_sequential_cnn,\n",
    "    build_functional_inception_cnn,\n",
    "    EarlyStoppingCallback,\n",
    "    LearningRateSchedulerCallback,\n",
    "    train_model_with_config,\n",
    "    run_grid_search\n",
    ")\n",
    "\n",
    "# Import utility functions\n",
    "from utils import (\n",
    "    load_mnist_data,\n",
    "    plot_training_history,\n",
    "    plot_sample_predictions,\n",
    "    plot_grid_search_results,\n",
    "    plot_overfitting_comparison,\n",
    "    compare_optimizers\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading and Exploration\n",
    "\n",
    "Load MNIST dataset with CPU-feasible subset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (using subset for CPU-feasible training)\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = load_mnist_data(\n",
    "    train_samples=10000,  # Subset of full 60k training set\n",
    "    val_samples=2000,\n",
    "    test_samples=10000    # Full test set\n",
    ")\n",
    "\n",
    "print(f\"Training set: X_train.shape = {X_train.shape}, y_train.shape = {y_train.shape}\")\n",
    "print(f\"Validation set: X_val.shape = {X_val.shape}, y_val.shape = {y_val.shape}\")\n",
    "print(f\"Test set: X_test.shape = {X_test.shape}, y_test.shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images using matplotlib\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = X_train[i].reshape(28, 28)\n",
    "    label = np.argmax(y_train[i])\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'Label: {label}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample MNIST Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Model Architecture Comparison\n",
    "\n",
    "Build and compare sequential and functional CNN architectures.\n",
    "\n",
    "### Task 2.1: Build Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequential model\n",
    "model_sequential = build_sequential_cnn()\n",
    "model_sequential.build(input_shape=(None, 28, 28, 1))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SEQUENTIAL CNN ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "model_sequential.summary()\n",
    "print(f\"\\nTotal parameters: {model_sequential.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build functional model with inception module\n",
    "model_functional = build_functional_inception_cnn()\n",
    "model_functional.build(input_shape=(None, 28, 28, 1))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FUNCTIONAL CNN WITH INCEPTION MODULE\")\n",
    "print(\"=\"*60)\n",
    "model_functional.summary()\n",
    "print(f\"\\nTotal parameters: {model_functional.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Analysis Questions\n",
    "\n",
    "Answer the following questions about your architectures:\n",
    "\n",
    "**Q1: Why is the Functional API necessary for the inception module? Could you build this with Sequential API?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q2: How do the parameter counts compare? Why?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q3: Explain how padding='same' vs. padding='valid' affects the output shape through your CNN layers.**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q4: What are the trade-offs between these two architectures in terms of complexity and expressiveness?**\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Optimizer Comparison\n",
    "\n",
    "Compare different optimizers on the same architecture.\n",
    "\n",
    "### Task 3.1: Design and Run Optimizer Comparison\n",
    "\n",
    "**Your Design Choices:**\n",
    "- Choose 2-3 optimizers to compare (e.g., SGD, Adam, RMSprop, Adagrad)\n",
    "- Choose a learning rate to use for all optimizers (for fair comparison)\n",
    "- Choose batch size and number of epochs\n",
    "\n",
    "**Justify your choices below:**\n",
    "\n",
    "*Why did you choose these optimizers? What learning rate makes sense? How many epochs?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your experimental parameters\n",
    "optimizers_to_test = []  # Your choices: e.g., ['sgd', 'adam', 'rmsprop']\n",
    "learning_rate = 0.001    # Your choice: what LR makes sense?\n",
    "batch_size = 32          # Your choice\n",
    "epochs = 20              # Your choice: enough to see convergence?\n",
    "\n",
    "# Run the comparison\n",
    "optimizer_histories = {}\n",
    "\n",
    "for opt_name in optimizers_to_test:\n",
    "    print(f\"\\nTraining with {opt_name.upper()}...\")\n",
    "    \n",
    "    # Build fresh model\n",
    "    model = build_sequential_cnn()\n",
    "    \n",
    "    # Train model\n",
    "    history = train_model_with_config(\n",
    "        model, X_train, y_train, X_val, y_val,\n",
    "        optimizer_name=opt_name,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    optimizer_histories[opt_name] = history\n",
    "    print(f\"Final val accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimizer comparison using matplotlib\n",
    "fig = compare_optimizers(optimizer_histories, metrics=['loss', 'accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Optimizer Analysis Questions\n",
    "\n",
    "**Q1: Which optimizer converged fastest? Provide evidence from your training curves.**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q2: Which optimizer achieved the best final validation accuracy? Why do you think this happened?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q3: Did any optimizer show signs of instability or oscillation? What might cause this?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q4: Based on your results, which optimizer would you recommend for this problem? Justify your choice.**\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Hyperparameter Grid Search\n",
    "\n",
    "Systematically explore hyperparameter combinations.\n",
    "\n",
    "### Task 4.1: Design Your Grid Search\n",
    "\n",
    "**Your Task:** Design a grid search with 6-12 total configurations.\n",
    "\n",
    "**Constraints:**\n",
    "- CPU-feasible: Keep total training time under 20-30 minutes\n",
    "- Meaningful: Explore parameters that might actually matter\n",
    "\n",
    "**Consider:**\n",
    "- Which optimizers to compare?\n",
    "- What learning rate ranges make sense?\n",
    "- Does batch size matter for this problem?\n",
    "- How many epochs per configuration?\n",
    "\n",
    "**Document your design rationale:**\n",
    "\n",
    "*Why did you choose these parameters and ranges? What are you trying to learn?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design your parameter grid (aim for 6-12 total configurations)\n",
    "param_grid = {\n",
    "    'optimizer': [],        # Your choices\n",
    "    'learning_rate': [],    # Your choices\n",
    "    'batch_size': []        # Your choices\n",
    "}\n",
    "\n",
    "# Calculate total configurations\n",
    "from itertools import product\n",
    "n_configs = len(list(product(*param_grid.values())))\n",
    "print(f\"Total configurations: {n_configs}\")\n",
    "\n",
    "# Use smaller subset for grid search to save time\n",
    "X_train_small = X_train[:5000]\n",
    "y_train_small = y_train[:5000]\n",
    "\n",
    "# TODO: Choose number of epochs (balance speed vs. convergence)\n",
    "grid_search_epochs = 15  # Your choice\n",
    "\n",
    "print(f\"Running grid search over {n_configs} configurations...\")\n",
    "print(f\"Estimated time: ~{n_configs * grid_search_epochs * 0.5 / 60:.1f} minutes on CPU\\n\")\n",
    "\n",
    "results = run_grid_search(\n",
    "    build_sequential_cnn,\n",
    "    X_train_small, y_train_small,\n",
    "    X_val, y_val,\n",
    "    param_grid,\n",
    "    epochs=grid_search_epochs,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Grid search complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a table\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Adapt this to your result structure\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Grid Search Results:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results using matplotlib\n",
    "# TODO: Choose which parameters to visualize and which metric to focus on\n",
    "fig = plot_grid_search_results(\n",
    "    results,\n",
    "    x_param='learning_rate',  # Adjust based on your param_grid\n",
    "    color_param='optimizer',   # Adjust based on your param_grid\n",
    "    metric='final_val_acc'     # Or 'final_val_loss'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Grid Search Analysis\n",
    "\n",
    "**Q1: Which configuration achieved the best validation performance? Report all hyperparameters.**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q2: How did learning rate affect performance? Provide specific examples from your results.**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q3: What interactions did you observe between hyperparameters (e.g., does the best learning rate depend on the optimizer)?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q4: If you were to run a follow-up grid search, what would you explore and why?**\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Overfitting Analysis\n",
    "\n",
    "Observe, induce, and mitigate overfitting through controlled experiments.\n",
    "\n",
    "### Task 5.1: Baseline Training\n",
    "\n",
    "First, establish a baseline with reasonable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose training parameters for baseline\n",
    "baseline_optimizer = 'adam'      # Your choice\n",
    "baseline_lr = 0.001              # Your choice\n",
    "baseline_epochs = 30             # Your choice\n",
    "\n",
    "print(f\"Training baseline model...\")\n",
    "print(f\"Optimizer: {baseline_optimizer}, LR: {baseline_lr}, Epochs: {baseline_epochs}\\n\")\n",
    "\n",
    "model_baseline = build_sequential_cnn()\n",
    "\n",
    "history_baseline = train_model_with_config(\n",
    "    model_baseline,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    optimizer_name=baseline_optimizer,\n",
    "    learning_rate=baseline_lr,\n",
    "    batch_size=32,\n",
    "    epochs=baseline_epochs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate train/val gap\n",
    "final_train_acc = history_baseline.history['accuracy'][-1]\n",
    "final_val_acc = history_baseline.history['val_accuracy'][-1]\n",
    "gap_baseline = final_train_acc - final_val_acc\n",
    "\n",
    "print(f\"\\nBaseline Results:\")\n",
    "print(f\"Final train accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final val accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"Train/Val gap: {gap_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline training curves\n",
    "fig = plot_training_history(history_baseline, title=\"Baseline Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Induce Worse Overfitting\n",
    "\n",
    "**Your Task:** Deliberately create worse overfitting.\n",
    "\n",
    "**Options to consider:**\n",
    "- Reduce training data (e.g., use only 500-2000 samples)\n",
    "- Increase model complexity (add more layers/filters)\n",
    "- Train for many more epochs\n",
    "- Increase learning rate\n",
    "\n",
    "**Document your approach:**\n",
    "\n",
    "*How will you induce overfitting? Why did you choose this approach?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design your overfitting experiment\n",
    "# Option 1: Reduce training data\n",
    "overfit_train_size = 1000  # Your choice (e.g., 500, 1000, 2000)\n",
    "X_train_overfit = X_train[:overfit_train_size]\n",
    "y_train_overfit = y_train[:overfit_train_size]\n",
    "\n",
    "# Option 2: Or modify other parameters\n",
    "overfit_epochs = 30       # Your choice\n",
    "overfit_lr = 0.001        # Your choice\n",
    "\n",
    "print(f\"Inducing overfitting...\")\n",
    "print(f\"Strategy: Training set reduced to {overfit_train_size} samples\\n\")\n",
    "\n",
    "model_overfit = build_sequential_cnn()\n",
    "\n",
    "history_overfit = train_model_with_config(\n",
    "    model_overfit,\n",
    "    X_train_overfit, y_train_overfit,\n",
    "    X_val, y_val,\n",
    "    optimizer_name=baseline_optimizer,\n",
    "    learning_rate=overfit_lr,\n",
    "    batch_size=32,\n",
    "    epochs=overfit_epochs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate train/val gap\n",
    "final_train_acc_overfit = history_overfit.history['accuracy'][-1]\n",
    "final_val_acc_overfit = history_overfit.history['val_accuracy'][-1]\n",
    "gap_overfit = final_train_acc_overfit - final_val_acc_overfit\n",
    "\n",
    "print(f\"\\nOverfitting Results:\")\n",
    "print(f\"Final train accuracy: {final_train_acc_overfit:.4f}\")\n",
    "print(f\"Final val accuracy: {final_val_acc_overfit:.4f}\")\n",
    "print(f\"Train/Val gap: {gap_overfit:.4f}\")\n",
    "print(f\"\\nGap increased from {gap_baseline:.4f} to {gap_overfit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.3: Mitigate Overfitting with Early Stopping\n",
    "\n",
    "**Your Task:** Choose early stopping parameters to mitigate overfitting.\n",
    "\n",
    "**Consider:**\n",
    "- What patience value makes sense?\n",
    "- Should you monitor loss or accuracy?\n",
    "- What about min_delta?\n",
    "\n",
    "**Document your choices:**\n",
    "\n",
    "*Why did you choose these early stopping parameters?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design your early stopping strategy\n",
    "es_monitor = 'val_loss'          # Your choice: 'val_loss' or 'val_accuracy'?\n",
    "es_patience = 5                  # Your choice: how patient should we be?\n",
    "es_min_delta = 0.0               # Your choice: minimum improvement to count?\n",
    "\n",
    "print(f\"Training with early stopping...\")\n",
    "print(f\"Monitor: {es_monitor}, Patience: {es_patience}, Min Delta: {es_min_delta}\\n\")\n",
    "\n",
    "model_early_stop = build_sequential_cnn()\n",
    "\n",
    "# Create early stopping callback with your parameters\n",
    "early_stop_callback = EarlyStoppingCallback(\n",
    "    monitor=es_monitor,\n",
    "    patience=es_patience,\n",
    "    min_delta=es_min_delta,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history_early_stop = train_model_with_config(\n",
    "    model_early_stop,\n",
    "    X_train_overfit, y_train_overfit,  # Same overfitting scenario\n",
    "    X_val, y_val,\n",
    "    optimizer_name=baseline_optimizer,\n",
    "    learning_rate=overfit_lr,\n",
    "    batch_size=32,\n",
    "    epochs=50,  # Set high - early stopping will kick in\n",
    "    callbacks=[early_stop_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "epochs_trained = len(history_early_stop.history['loss'])\n",
    "final_train_acc_es = history_early_stop.history['accuracy'][-1]\n",
    "final_val_acc_es = history_early_stop.history['val_accuracy'][-1]\n",
    "gap_early_stop = final_train_acc_es - final_val_acc_es\n",
    "\n",
    "print(f\"\\nEarly Stopping Results:\")\n",
    "print(f\"Stopped at epoch: {epochs_trained}/50\")\n",
    "print(f\"Final train accuracy: {final_train_acc_es:.4f}\")\n",
    "print(f\"Final val accuracy: {final_val_acc_es:.4f}\")\n",
    "print(f\"Train/Val gap: {gap_early_stop:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three scenarios using matplotlib\n",
    "scenarios = {\n",
    "    f'Baseline ({len(X_train)} samples)': history_baseline,\n",
    "    f'Overfitting ({overfit_train_size} samples)': history_overfit,\n",
    "    f'With Early Stopping (patience={es_patience})': history_early_stop\n",
    "}\n",
    "\n",
    "fig = plot_overfitting_comparison(scenarios, metric='accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.4: Overfitting Analysis Questions\n",
    "\n",
    "**Q1: Describe the overfitting you observed in your baseline model. What evidence supports your conclusion?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q2: How effective was your strategy for inducing overfitting? Explain the mechanism.**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q3: How effective was early stopping at mitigating overfitting? Support your answer with specific metrics.**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q4: At what point during training did overfitting become severe? How can you tell from the curves?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q5: What other strategies could you use to reduce overfitting in this scenario?**\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Callback Effectiveness\n",
    "\n",
    "Analyze the impact of learning rate scheduling.\n",
    "\n",
    "### Task 6.1: Design Learning Rate Schedule\n",
    "\n",
    "**Your Task:** Choose LR scheduling parameters.\n",
    "\n",
    "**Consider:**\n",
    "- What initial learning rate?\n",
    "- How aggressively should LR decay (decay_rate)?\n",
    "- How often should it decay (decay_steps)?\n",
    "- Which optimizer benefits most from scheduling?\n",
    "\n",
    "**Document your design:**\n",
    "\n",
    "*Why did you choose these scheduling parameters?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design your LR scheduling experiment\n",
    "lr_initial = 0.01        # Your choice\n",
    "lr_decay_rate = 0.5      # Your choice (e.g., 0.5 = halve, 0.1 = reduce by 10x)\n",
    "lr_decay_steps = 10      # Your choice (decay every N epochs)\n",
    "lr_optimizer = 'sgd'     # Your choice (SGD often benefits most)\n",
    "\n",
    "print(f\"Training with learning rate scheduler...\")\n",
    "print(f\"Initial LR: {lr_initial}, Decay rate: {lr_decay_rate}, Decay every: {lr_decay_steps} epochs\\n\")\n",
    "\n",
    "model_lr_schedule = build_sequential_cnn()\n",
    "\n",
    "# Create LR scheduler callback\n",
    "lr_scheduler = LearningRateSchedulerCallback(\n",
    "    initial_lr=lr_initial,\n",
    "    decay_rate=lr_decay_rate,\n",
    "    decay_steps=lr_decay_steps\n",
    ")\n",
    "\n",
    "history_lr_schedule = train_model_with_config(\n",
    "    model_lr_schedule,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    optimizer_name=lr_optimizer,\n",
    "    learning_rate=lr_initial,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nLR Schedule completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LR scheduling effect\n",
    "fig = plot_training_history(\n",
    "    history_lr_schedule, \n",
    "    title=f\"Training with LR Decay (rate={lr_decay_rate}, every {lr_decay_steps} epochs)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.2: Callback Analysis Questions\n",
    "\n",
    "**Q1: How did you choose your early stopping patience? What factors did you consider?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q2: Did the learning rate scheduler improve convergence compared to fixed LR? Provide evidence.**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q3: How would you choose decay_rate and decay_steps in practice for a new problem?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**Q4: Could you combine early stopping and LR scheduling? What might be the benefits and challenges?**\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Executive Summary\n",
    "\n",
    "### Task 7.1: Key Findings\n",
    "\n",
    "Summarize your key findings from this assignment in 3-5 bullet points:\n",
    "\n",
    "*Your summary here*\n",
    "\n",
    "### Task 7.2: Practical Recommendations\n",
    "\n",
    "Based on your experiments, what recommendations would you give to someone training CNNs for image classification?\n",
    "\n",
    "*Your recommendations here*\n",
    "\n",
    "### Task 7.3: Reflection\n",
    "\n",
    "What was the most surprising or interesting finding from your experiments?\n",
    "\n",
    "*Your reflection here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Peer Review Preparation\n",
    "\n",
    "### For your peer reviewers:\n",
    "\n",
    "**What aspect of your analysis are you most proud of?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**What question or uncertainty would you like feedback on?**\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "**What was the most challenging part of this assignment?**\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, ensure you have:\n",
    "\n",
    "- [ ] Completed all code implementations in `student_code.py`\n",
    "- [ ] All tests passing (`python -m pytest tests.py`)\n",
    "- [ ] Filled in all TODO sections in this notebook with your design choices\n",
    "- [ ] Justified your experimental design decisions\n",
    "- [ ] Answered all analysis questions\n",
    "- [ ] Created clear matplotlib visualizations (no seaborn)\n",
    "- [ ] Written executive summary and reflections\n",
    "- [ ] Exported this notebook as PDF for peer review\n",
    "- [ ] Double-checked that all cells run without errors\n",
    "\n",
    "**Note:** Export this notebook as PDF via: File → Save and Export Notebook As → PDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
