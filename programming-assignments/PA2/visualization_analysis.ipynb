{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA2: Gradient Descent Visualization & Analysis\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand the impact of hyperparameters on gradient descent behavior\n",
    "- Visualize optimization landscapes and convergence patterns\n",
    "- Develop intuition for numerical stability vs. accuracy trade-offs\n",
    "- Practice critical analysis and communication of machine learning results\n",
    "\n",
    "**Submission Requirements:**\n",
    "- Complete all code cells and written analysis sections\n",
    "- Export notebook as PDF for peer review\n",
    "- Focus on clear explanations and insightful visualizations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from student_code import (\n",
    "    numerical_derivative,\n",
    "    numerical_gradient,\n",
    "    linear_predict,\n",
    "    mse_cost,\n",
    "    mse_gradient,\n",
    "    initialize_weights,\n",
    "    gradient_descent_step,\n",
    "    gradient_descent,\n",
    "    add_intercept,\n",
    "    generate_synthetic_data,\n",
    "    has_converged\n",
    ")\n",
    "\n",
    "# Set plotting style for clean visualizations\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"All functions imported successfully!\")\n",
    "print(\"Ready for gradient descent analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Delta-h Sensitivity Analysis\n",
    "\n",
    "**Objective**: Explore how the step size `h` in numerical derivatives affects both accuracy and numerical stability.\n",
    "\n",
    "**Key Questions:**\n",
    "- What happens when `h` is too large? Too small?\n",
    "- How do we balance numerical precision with approximation error?\n",
    "- Why does this matter for gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test function where we know the exact derivative\n",
    "def test_function(x):\n",
    "    \"\"\"f(x) = x^3 + 2*x^2 + x, so f'(x) = 3*x^2 + 4*x + 1\"\"\"\n",
    "    return x[0]**3 + 2*x[0]**2 + x[0]\n",
    "\n",
    "def true_derivative(x_val):\n",
    "    \"\"\"Analytical derivative of test_function\"\"\"\n",
    "    return 3*x_val**2 + 4*x_val + 1\n",
    "\n",
    "# Test point\n",
    "x_test = np.array([2.0])\n",
    "true_deriv = true_derivative(x_test[0])\n",
    "\n",
    "print(f\"Test point: x = {x_test[0]}\")\n",
    "print(f\"True derivative: f'({x_test[0]}) = {true_deriv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different h values\n",
    "h_values = np.logspace(-12, 0, 50)  # From 1e-12 to 1\n",
    "numerical_derivatives = []\n",
    "errors = []\n",
    "\n",
    "for h in h_values:\n",
    "    numerical_deriv = numerical_derivative(test_function, x_test, dimension=0, h=h)\n",
    "    error = abs(numerical_deriv - true_deriv)\n",
    "    \n",
    "    numerical_derivatives.append(numerical_deriv)\n",
    "    errors.append(error)\n",
    "\n",
    "numerical_derivatives = np.array(numerical_derivatives)\n",
    "errors = np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Numerical derivative vs h\n",
    "ax1.semilogx(h_values, numerical_derivatives, 'b-', linewidth=2, label='Numerical Derivative')\n",
    "ax1.axhline(y=true_deriv, color='r', linestyle='--', linewidth=2, label='True Derivative')\n",
    "ax1.set_xlabel('Step Size (h)')\n",
    "ax1.set_ylabel('Derivative Value')\n",
    "ax1.set_title('Numerical Derivative vs Step Size')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error vs h\n",
    "ax2.loglog(h_values, errors, 'g-', linewidth=2, label='Absolute Error')\n",
    "ax2.set_xlabel('Step Size (h)')\n",
    "ax2.set_ylabel('Absolute Error')\n",
    "ax2.set_title('Numerical Derivative Error vs Step Size')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal h\n",
    "optimal_idx = np.argmin(errors)\n",
    "optimal_h = h_values[optimal_idx]\n",
    "min_error = errors[optimal_idx]\n",
    "\n",
    "print(f\"\\nOptimal h: {optimal_h:.2e}\")\n",
    "print(f\"Minimum error: {min_error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Delta-h Analysis Questions\n",
    "\n",
    "**Answer the following based on your plots above:**\n",
    "\n",
    "1. **Large h behavior**: What happens to numerical derivative accuracy when h is too large (h > 0.1)? Why?\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "2. **Small h behavior**: What happens when h becomes very small (h < 1e-10)? What causes this?\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "3. **Sweet spot**: Why is there an optimal h value around 1e-5 to 1e-6? What two types of error are being balanced?\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "4. **Practical implications**: How does this analysis inform your choice of h in gradient descent? What could go wrong if you choose h poorly?\n",
    "\n",
    "*Your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Learning Rate Exploration\n",
    "\n",
    "**Objective**: Understand how learning rate affects gradient descent convergence, stability, and speed.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Convergence speed vs. stability trade-offs\n",
    "- Oscillation and divergence patterns\n",
    "- The \"Goldilocks zone\" for learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset for consistent comparison\n",
    "np.random.seed(42)  # Fixed seed for reproducible analysis\n",
    "X, y = generate_synthetic_data(100, 2, noise=0.1, seed=42)\n",
    "X_with_intercept = add_intercept(X)\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"X with intercept shape: {X_with_intercept.shape}\")\n",
    "print(f\"Features range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "epochs = 200\n",
    "\n",
    "# Store results for each learning rate\n",
    "lr_results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    \n",
    "    final_weights, cost_history = gradient_descent(\n",
    "        X_with_intercept, y, learning_rate=lr, epochs=epochs\n",
    "    )\n",
    "    \n",
    "    lr_results[lr] = {\n",
    "        'weights': final_weights,\n",
    "        'costs': cost_history,\n",
    "        'final_cost': cost_history[-1],\n",
    "        'converged': has_converged(cost_history, tolerance=1e-6)\n",
    "    }\n",
    "    \n",
    "    print(f\"  Final cost: {cost_history[-1]:.6f}\")\n",
    "    print(f\"  Converged: {lr_results[lr]['converged']}\")\n",
    "    print(f\"  Final weights: [{final_weights[0]:.3f}, {final_weights[1]:.3f}, {final_weights[2]:.3f}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate effects\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(learning_rates)))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    costs = lr_results[lr]['costs']\n",
    "    \n",
    "    axes[i].plot(costs, color=colors[i], linewidth=2)\n",
    "    axes[i].set_title(f'Learning Rate: {lr}')\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "    axes[i].set_ylabel('Cost')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add convergence info\n",
    "    final_cost = lr_results[lr]['final_cost']\n",
    "    converged = lr_results[lr]['converged']\n",
    "    status = \"Converged\" if converged else \"Not Converged\"\n",
    "    axes[i].text(0.05, 0.95, f'Final: {final_cost:.4f}\\n{status}', \n",
    "                transform=axes[i].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all learning rates on one plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    costs = lr_results[lr]['costs']\n",
    "    plt.plot(costs, color=colors[i], linewidth=2, label=f'LR = {lr}')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Curves for Different Learning Rates')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to see all curves clearly\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LEARNING RATE COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'LR':<8} {'Final Cost':<12} {'Converged':<10} {'Final Weights':<25}\")\n",
    "print(\"-\"*60)\n",
    "for lr in learning_rates:\n",
    "    result = lr_results[lr]\n",
    "    weights_str = f\"[{result['weights'][0]:.2f}, {result['weights'][1]:.2f}, {result['weights'][2]:.2f}]\"\n",
    "    print(f\"{lr:<8} {result['final_cost']:<12.6f} {result['converged']!s:<10} {weights_str:<25}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Learning Rate Analysis Questions\n",
    "\n",
    "**Analyze your results and answer the following:**\n",
    "\n",
    "1. **Too small learning rate**: What happens with very small learning rates (0.001)? What are the pros and cons?\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "2. **Too large learning rate**: What happens with large learning rates (0.5)? Do you see oscillations or divergence?\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "3. **Optimal range**: Which learning rate(s) work best for this problem? What makes them \"just right\"?\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "4. **Practical guidelines**: Based on this analysis, how would you choose a learning rate for a new problem?\n",
    "\n",
    "*Your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Standardized Stopping Condition Analysis\n",
    "\n",
    "**Objective**: Using identical starting conditions, analyze when to stop gradient descent and justify your reasoning.\n",
    "\n",
    "**Setup**: Everyone will use the same data, initial weights, and learning rate. Your task is to determine the optimal stopping point and defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARDIZED SETUP - DO NOT MODIFY\n",
    "# Everyone uses identical conditions for fair comparison\n",
    "\n",
    "np.random.seed(12345)  # Fixed seed\n",
    "X_std, y_std = generate_synthetic_data(150, 3, noise=0.2, seed=12345)\n",
    "X_std_intercept = add_intercept(X_std)\n",
    "\n",
    "# Fixed initial conditions\n",
    "initial_weights = np.array([0.1, -0.1, 0.05, -0.05])  # [bias, w1, w2, w3]\n",
    "learning_rate = 0.02\n",
    "max_epochs = 2000\n",
    "\n",
    "print(\"STANDARDIZED CONDITIONS:\")\n",
    "print(f\"Dataset: {X_std.shape[0]} samples, {X_std.shape[1]} features\")\n",
    "print(f\"Initial weights: {initial_weights}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Maximum epochs: {max_epochs}\")\n",
    "print(\"\\nRunning gradient descent...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent with detailed tracking\n",
    "weights = initial_weights.copy()\n",
    "cost_history_std = []\n",
    "weight_history = [weights.copy()]\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    weights, cost = gradient_descent_step(X_std_intercept, y_std, weights, learning_rate)\n",
    "    cost_history_std.append(cost)\n",
    "    \n",
    "    # Store weights every 50 epochs for trajectory analysis\n",
    "    if epoch % 50 == 0:\n",
    "        weight_history.append(weights.copy())\n",
    "\n",
    "final_weights_std = weights\n",
    "print(f\"Training completed: {len(cost_history_std)} epochs\")\n",
    "print(f\"Final cost: {cost_history_std[-1]:.6f}\")\n",
    "print(f\"Final weights: {final_weights_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed cost curve analysis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Main cost curve\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(cost_history_std, 'b-', linewidth=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Full Training Cost Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Zoomed view of later epochs\n",
    "plt.subplot(2, 2, 2)\n",
    "start_idx = max(0, len(cost_history_std) - 500)\n",
    "plt.plot(range(start_idx, len(cost_history_std)), cost_history_std[start_idx:], 'r-', linewidth=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Final 500 Epochs (Detailed View)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Cost differences between consecutive epochs\n",
    "plt.subplot(2, 2, 3)\n",
    "cost_diffs = np.diff(cost_history_std)\n",
    "plt.plot(cost_diffs, 'g-', linewidth=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost Change')\n",
    "plt.title('Cost Change Per Epoch')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale for better visibility\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.semilogy(cost_history_std, 'purple', linewidth=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost (log scale)')\n",
    "plt.title('Cost Curve (Log Scale)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis tools for stopping condition\n",
    "def analyze_convergence_metrics(costs, window_size=100):\n",
    "    \"\"\"Calculate various convergence metrics\"\"\"\n",
    "    costs = np.array(costs)\n",
    "    \n",
    "    # Moving average of cost changes\n",
    "    cost_changes = np.abs(np.diff(costs))\n",
    "    moving_avg_change = np.convolve(cost_changes, np.ones(window_size)/window_size, mode='valid')\n",
    "    \n",
    "    # Relative improvement\n",
    "    rel_improvements = np.abs(np.diff(costs)) / (costs[:-1] + 1e-10)\n",
    "    \n",
    "    # Cost variance in recent window\n",
    "    recent_variance = []\n",
    "    for i in range(window_size, len(costs)):\n",
    "        window_costs = costs[i-window_size:i]\n",
    "        recent_variance.append(np.var(window_costs))\n",
    "    \n",
    "    return {\n",
    "        'moving_avg_change': moving_avg_change,\n",
    "        'relative_improvements': rel_improvements,\n",
    "        'recent_variance': np.array(recent_variance)\n",
    "    }\n",
    "\n",
    "metrics = analyze_convergence_metrics(cost_history_std, window_size=50)\n",
    "\n",
    "# Plot convergence metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Moving average of cost changes\n",
    "axes[0,0].plot(metrics['moving_avg_change'])\n",
    "axes[0,0].set_title('Moving Average of Cost Changes')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Average |Cost Change|')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Relative improvements\n",
    "axes[0,1].semilogy(metrics['relative_improvements'])\n",
    "axes[0,1].set_title('Relative Cost Improvements')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('|ΔCost| / Cost')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Recent variance\n",
    "axes[1,0].semilogy(metrics['recent_variance'])\n",
    "axes[1,0].set_title('Cost Variance in Recent Window')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Variance')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence test with different tolerances\n",
    "tolerances = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "convergence_epochs = []\n",
    "\n",
    "for tol in tolerances:\n",
    "    for epoch in range(10, len(cost_history_std)):\n",
    "        partial_history = cost_history_std[:epoch+1]\n",
    "        if has_converged(partial_history, tolerance=tol):\n",
    "            convergence_epochs.append(epoch)\n",
    "            break\n",
    "    else:\n",
    "        convergence_epochs.append(len(cost_history_std))\n",
    "\n",
    "axes[1,1].semilogx(tolerances, convergence_epochs, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1,1].set_title('Convergence Epoch vs Tolerance')\n",
    "axes[1,1].set_xlabel('Tolerance')\n",
    "axes[1,1].set_ylabel('Convergence Epoch')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"CONVERGENCE ANALYSIS:\")\n",
    "for i, tol in enumerate(tolerances):\n",
    "    epoch = convergence_epochs[i]\n",
    "    if epoch < len(cost_history_std):\n",
    "        print(f\"Tolerance {tol:.0e}: Converged at epoch {epoch} (cost: {cost_history_std[epoch]:.6f})\")\n",
    "    else:\n",
    "        print(f\"Tolerance {tol:.0e}: Did not converge within {max_epochs} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Your Stopping Condition Decision\n",
    "\n",
    "**Based on the standardized run above, determine your optimal stopping point and justify your choice.**\n",
    "\n",
    "#### Your Recommended Stopping Point:\n",
    "\n",
    "**Epoch:** *(Enter your chosen epoch here)*\n",
    "\n",
    "**Cost at stopping point:** *(Enter the cost at your chosen epoch)*\n",
    "\n",
    "**Tolerance threshold:** *(What tolerance would you recommend?)*\n",
    "\n",
    "#### Your Justification:\n",
    "\n",
    "**1. Why did you choose this stopping point?**\n",
    "\n",
    "*Your reasoning here - consider computational cost, diminishing returns, practical considerations*\n",
    "\n",
    "**2. What evidence from the plots supports your decision?**\n",
    "\n",
    "*Reference specific metrics, curves, or patterns that informed your choice*\n",
    "\n",
    "**3. What are the trade-offs of your choice?**\n",
    "\n",
    "*What do you gain and what do you potentially sacrifice with this stopping point?*\n",
    "\n",
    "**4. How would you defend this choice to a colleague?**\n",
    "\n",
    "*Present your argument as if explaining to another data scientist*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Comparative Studies\n",
    "\n",
    "**Objective**: Compare different aspects of gradient descent to understand their relative importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialization Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different weight initialization methods\n",
    "np.random.seed(42)  # For reproducible comparison\n",
    "X_init, y_init = generate_synthetic_data(80, 2, noise=0.15, seed=42)\n",
    "X_init_intercept = add_intercept(X_init)\n",
    "\n",
    "init_methods = ['zeros', 'random', 'small_random']\n",
    "init_results = {}\n",
    "epochs = 150\n",
    "lr = 0.05\n",
    "\n",
    "for method in init_methods:\n",
    "    print(f\"Testing initialization: {method}\")\n",
    "    \n",
    "    # Reset random seed before each method for fair comparison of random methods\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Initialize weights\n",
    "    if method == 'zeros':\n",
    "        initial_weights = initialize_weights(X_init_intercept.shape[1], method='zeros')\n",
    "    elif method == 'random':\n",
    "        initial_weights = initialize_weights(X_init_intercept.shape[1], method='random')\n",
    "    else:  # small_random\n",
    "        initial_weights = initialize_weights(X_init_intercept.shape[1], method='small_random')\n",
    "    \n",
    "    print(f\"  Initial weights: {initial_weights}\")\n",
    "    \n",
    "    # Run gradient descent manually to track from specific initial weights\n",
    "    weights = initial_weights.copy()\n",
    "    costs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        weights, cost = gradient_descent_step(X_init_intercept, y_init, weights, lr)\n",
    "        costs.append(cost)\n",
    "    \n",
    "    init_results[method] = {\n",
    "        'initial_weights': initial_weights,\n",
    "        'final_weights': weights,\n",
    "        'costs': costs,\n",
    "        'final_cost': costs[-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"  Final cost: {costs[-1]:.6f}\")\n",
    "    print(f\"  Final weights: {weights}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize initialization comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, method in enumerate(init_methods):\n",
    "    costs = init_results[method]['costs']\n",
    "    plt.plot(costs, color=colors[i], linewidth=2, label=f'{method.title()} Init')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Curves for Different Weight Initializations')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# Summary comparison\n",
    "print(\"INITIALIZATION COMPARISON:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Method':<15} {'Initial Weights':<25} {'Final Cost':<12} {'Convergence':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for method in init_methods:\n",
    "    result = init_results[method]\n",
    "    init_str = f\"[{result['initial_weights'][0]:.3f}, {result['initial_weights'][1]:.3f}, {result['initial_weights'][2]:.3f}]\"\n",
    "    converged = has_converged(result['costs'])\n",
    "    print(f\"{method:<15} {init_str:<25} {result['final_cost']:<12.6f} {converged!s:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Scaling Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with features on very different scales\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Feature 1: Small scale (0-1)\n",
    "feature1 = np.random.uniform(0, 1, n_samples)\n",
    "# Feature 2: Large scale (1000-2000) \n",
    "feature2 = np.random.uniform(1000, 2000, n_samples)\n",
    "\n",
    "X_unscaled = np.column_stack([feature1, feature2])\n",
    "true_weights = np.array([2.0, 0.001])  # Compensate for scale difference\n",
    "y_scale = X_unscaled @ true_weights + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "# Create scaled version (standardization)\n",
    "X_scaled = (X_unscaled - X_unscaled.mean(axis=0)) / X_unscaled.std(axis=0)\n",
    "\n",
    "# Add intercepts\n",
    "X_unscaled_intercept = add_intercept(X_unscaled)\n",
    "X_scaled_intercept = add_intercept(X_scaled)\n",
    "\n",
    "print(\"FEATURE SCALING EXPERIMENT:\")\n",
    "print(f\"Unscaled features - Feature 1 range: [{feature1.min():.3f}, {feature1.max():.3f}]\")\n",
    "print(f\"Unscaled features - Feature 2 range: [{feature2.min():.0f}, {feature2.max():.0f}]\")\n",
    "print(f\"Scaled features - Feature 1 range: [{X_scaled[:,0].min():.3f}, {X_scaled[:,0].max():.3f}]\")\n",
    "print(f\"Scaled features - Feature 2 range: [{X_scaled[:,1].min():.3f}, {X_scaled[:,1].max():.3f}]\")\n",
    "print(f\"Target range: [{y_scale.min():.3f}, {y_scale.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on both scaled and unscaled data\n",
    "epochs = 300\n",
    "lr = 0.01  # Same learning rate for both\n",
    "\n",
    "# Unscaled training\n",
    "print(\"Training on unscaled features...\")\n",
    "weights_unscaled, costs_unscaled = gradient_descent(\n",
    "    X_unscaled_intercept, y_scale, learning_rate=lr, epochs=epochs\n",
    ")\n",
    "\n",
    "# Scaled training  \n",
    "print(\"Training on scaled features...\")\n",
    "weights_scaled, costs_scaled = gradient_descent(\n",
    "    X_scaled_intercept, y_scale, learning_rate=lr, epochs=epochs\n",
    ")\n",
    "\n",
    "print(f\"\\nUnscaled - Final cost: {costs_unscaled[-1]:.6f}\")\n",
    "print(f\"Scaled - Final cost: {costs_scaled[-1]:.6f}\")\n",
    "print(f\"\\nUnscaled - Final weights: {weights_unscaled}\")\n",
    "print(f\"Scaled - Final weights: {weights_scaled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature scaling impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Cost curves comparison\n",
    "ax1.plot(costs_unscaled, 'r-', linewidth=2, label='Unscaled Features')\n",
    "ax1.plot(costs_scaled, 'b-', linewidth=2, label='Scaled Features')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Cost')\n",
    "ax1.set_title('Feature Scaling Impact on Convergence')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Gradient magnitudes (approximate)\n",
    "# Calculate gradients at several points during training\n",
    "sample_epochs = range(0, epochs, 20)\n",
    "unscaled_grad_norms = []\n",
    "scaled_grad_norms = []\n",
    "\n",
    "for epoch in sample_epochs:\n",
    "    # Reconstruct weights at this epoch (approximate)\n",
    "    w_unscaled = initialize_weights(3, 'zeros')\n",
    "    w_scaled = initialize_weights(3, 'zeros')\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        w_unscaled, _ = gradient_descent_step(X_unscaled_intercept, y_scale, w_unscaled, lr)\n",
    "        w_scaled, _ = gradient_descent_step(X_scaled_intercept, y_scale, w_scaled, lr)\n",
    "    \n",
    "    # Calculate gradient norms\n",
    "    grad_unscaled = mse_gradient(X_unscaled_intercept, y_scale, w_unscaled)\n",
    "    grad_scaled = mse_gradient(X_scaled_intercept, y_scale, w_scaled)\n",
    "    \n",
    "    unscaled_grad_norms.append(np.linalg.norm(grad_unscaled))\n",
    "    scaled_grad_norms.append(np.linalg.norm(grad_scaled))\n",
    "\n",
    "ax2.plot(sample_epochs, unscaled_grad_norms, 'r-', linewidth=2, label='Unscaled Features')\n",
    "ax2.plot(sample_epochs, scaled_grad_norms, 'b-', linewidth=2, label='Scaled Features') \n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Gradient Norm')\n",
    "ax2.set_title('Gradient Magnitude During Training')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comparative Analysis Questions\n",
    "\n",
    "**1. Initialization Methods:**\n",
    "\n",
    "a) Which initialization method performed best? Why do you think this happened?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "b) When might different initialization strategies be preferred?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "**2. Feature Scaling:**\n",
    "\n",
    "a) How did feature scaling affect convergence speed and stability?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "b) Why does feature scaling matter for gradient descent? (Think about the optimization landscape)\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "c) What practical advice would you give about feature scaling?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Executive Summary & Communication\n",
    "\n",
    "**Objective**: Synthesize your findings into a clear, professional summary suitable for peer review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Executive Summary\n",
    "\n",
    "**Write a 3-4 paragraph executive summary of your gradient descent analysis for a technical audience:**\n",
    "\n",
    "#### Overview & Methodology\n",
    "*Describe what you analyzed and how*\n",
    "\n",
    "#### Key Findings\n",
    "*Summarize your most important discoveries about hyperparameters, convergence, etc.*\n",
    "\n",
    "#### Practical Recommendations  \n",
    "*What actionable advice would you give to someone implementing gradient descent?*\n",
    "\n",
    "#### Implications\n",
    "*Why do these findings matter for machine learning practitioners?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Questions for Peer Reviewers\n",
    "\n",
    "**Provide 3 specific questions you'd like your peers to address when reviewing your analysis:**\n",
    "\n",
    "1. **Question about stopping condition:** \n",
    "\n",
    "2. **Question about hyperparameter insights:**\n",
    "\n",
    "3. **Question about practical applications:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Reflection on Learning\n",
    "\n",
    "**Reflect on what you learned from implementing gradient descent from scratch:**\n",
    "\n",
    "#### Technical Insights\n",
    "*What did you understand better after implementing the math yourself?*\n",
    "\n",
    "#### Challenges Encountered\n",
    "*What was difficult? How did you overcome challenges?*\n",
    "\n",
    "#### Connections to Broader ML\n",
    "*How does this foundational understanding help with more complex ML algorithms?*\n",
    "\n",
    "#### Future Applications\n",
    "*How will you apply these insights to future projects?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, verify you have completed:\n",
    "\n",
    "**Part 1: Delta-h Analysis**\n",
    "- [ ] Generated delta-h sensitivity plots\n",
    "- [ ] Answered all analysis questions about numerical stability\n",
    "- [ ] Identified optimal h value and explained trade-offs\n",
    "\n",
    "**Part 2: Learning Rate Exploration**  \n",
    "- [ ] Tested multiple learning rates with visualizations\n",
    "- [ ] Analyzed convergence patterns and identified optimal range\n",
    "- [ ] Answered questions about learning rate selection\n",
    "\n",
    "**Part 3: Standardized Stopping Analysis**\n",
    "- [ ] Used identical setup for fair comparison\n",
    "- [ ] Analyzed convergence metrics and stopping criteria\n",
    "- [ ] Made specific stopping recommendation with justification\n",
    "\n",
    "**Part 4: Comparative Studies**\n",
    "- [ ] Compared initialization methods with analysis\n",
    "- [ ] Demonstrated feature scaling impact\n",
    "- [ ] Answered comparative analysis questions\n",
    "\n",
    "**Part 5: Communication**\n",
    "- [ ] Written professional executive summary\n",
    "- [ ] Provided specific questions for peer review\n",
    "- [ ] Reflected on learning and broader applications\n",
    "\n",
    "**Export your notebook as PDF for peer review submission.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}