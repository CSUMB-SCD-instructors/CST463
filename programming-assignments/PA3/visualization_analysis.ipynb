{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA3: Backpropagation & Neural Networks - Analysis Notebook\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this analysis, you will:\n",
    "\n",
    "1. **Analyze gradient flow** through neural networks using concrete numerical examples\n",
    "2. **Compare network architectures** on a challenging nonlinear classification problem\n",
    "3. **Diagnose training dynamics** and identify common failure modes\n",
    "4. **Validate your implementations** using gradient checking techniques\n",
    "5. **Develop professional ML skills** for real-world neural network debugging\n",
    "\n",
    "## Assignment Structure\n",
    "\n",
    "You will work with a \"Swiss Roll\" dataset that demonstrates the limitations of linear methods and the power of neural networks. Your task is to analyze how network complexity affects performance and to develop insights about training dynamics.\n",
    "\n",
    "**Your contributions**: Choose appropriate visualizations, interpret results, and write analytical summaries explaining your findings. The helper functions are provided, but the analysis choices and interpretations are yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import your implementations\n",
    "from student_code import *\n",
    "from utils import *\n",
    "\n",
    "# Set style for professional plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "The following functions are provided to support your analysis. You should use these tools to explore different aspects of neural network training, but the choice of what to analyze and how to interpret results is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_swiss_roll_2d(n_samples=300, noise=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Generate 2D Swiss Roll classification data.\n",
    "    This creates a nonlinear classification problem.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate spiral pattern\n",
    "    t = np.random.uniform(0, 4*np.pi, n_samples)\n",
    "    x = t * np.cos(t) + noise * np.random.randn(n_samples)\n",
    "    y = t * np.sin(t) + noise * np.random.randn(n_samples)\n",
    "    \n",
    "    # Create classification based on radius\n",
    "    radius = np.sqrt(x**2 + y**2)\n",
    "    labels = (radius > np.median(radius)).astype(int)\n",
    "    \n",
    "    X = np.column_stack([x, y])\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)  # Normalize\n",
    "    \n",
    "    return X, labels\n",
    "\n",
    "def plot_decision_boundary(X, y, predict_func, title=\"Decision Boundary\", ax=None):\n",
    "    \"\"\"\n",
    "    Plot 2D data with decision boundary.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix (n_samples, 2)\n",
    "    - y: Labels (n_samples,)\n",
    "    - predict_func: Function that takes X and returns predictions\n",
    "    - title: Plot title\n",
    "    - ax: Matplotlib axis (optional)\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    \n",
    "    # Plot data points\n",
    "    colors = ['red', 'blue']\n",
    "    for i in range(2):\n",
    "        mask = y == i\n",
    "        ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, s=50,\n",
    "                  label=f'Class {i}', edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Create decision boundary\n",
    "    if predict_func is not None:\n",
    "        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                            np.linspace(y_min, y_max, 100))\n",
    "        \n",
    "        mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z = predict_func(mesh_points)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        ax.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)\n",
    "        ax.contourf(xx, yy, Z, levels=50, alpha=0.3, cmap='RdYlBu')\n",
    "    \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "def train_simple_network(X, y, hidden_size=0, learning_rate=0.1, epochs=300, activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Train a neural network with 0 or 1 hidden layer.\n",
    "    \n",
    "    Parameters:\n",
    "    - hidden_size: 0 for single layer, >0 for two-layer network\n",
    "    - Returns: weights dict, loss history, prediction function\n",
    "    \"\"\"\n",
    "    y_formatted = y.reshape(-1, 1)\n",
    "    \n",
    "    if hidden_size == 0:\n",
    "        # Single layer network\n",
    "        weights, loss_history = train_single_layer(\n",
    "            X, y_formatted, activation=activation, loss_type='mse',\n",
    "            epochs=epochs, learning_rate=learning_rate\n",
    "        )\n",
    "        \n",
    "        def predict_func(X_new):\n",
    "            u, v = single_layer_forward(X_new, weights['W'], weights['b'], activation)\n",
    "            return v.flatten()\n",
    "            \n",
    "        return weights, loss_history, predict_func\n",
    "    \n",
    "    else:\n",
    "        # Two layer network (manual implementation)\n",
    "        np.random.seed(42)\n",
    "        W1 = np.random.randn(X.shape[1], hidden_size) * 0.1\n",
    "        b1 = np.zeros(hidden_size)\n",
    "        W2 = np.random.randn(hidden_size, 1) * 0.1\n",
    "        b2 = np.zeros(1)\n",
    "        \n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            u1, v1 = single_layer_forward(X, W1, b1, activation)\n",
    "            u2, v2 = single_layer_forward(v1, W2, b2, 'sigmoid')\n",
    "            \n",
    "            loss = mse_loss(y_formatted, v2)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            dL_dv2 = mse_derivative(y_formatted, v2)\n",
    "            dL_dW2, dL_db2, dL_dv1 = single_layer_backward(dL_dv2, u2, v1, W2, 'sigmoid')\n",
    "            dL_dW1, dL_db1, dL_dX = single_layer_backward(dL_dv1, u1, X, W1, activation)\n",
    "            \n",
    "            # Update weights\n",
    "            W1 -= learning_rate * dL_dW1\n",
    "            b1 -= learning_rate * dL_db1\n",
    "            W2 -= learning_rate * dL_dW2\n",
    "            b2 -= learning_rate * dL_db2\n",
    "        \n",
    "        weights = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        def predict_func(X_new):\n",
    "            u1, v1 = single_layer_forward(X_new, W1, b1, activation)\n",
    "            u2, v2 = single_layer_forward(v1, W2, b2, 'sigmoid')\n",
    "            return v2.flatten()\n",
    "        \n",
    "        return weights, loss_history, predict_func\n",
    "\n",
    "def demonstrate_chain_rule_step_by_step(X_sample, y_sample, weights):\n",
    "    \"\"\"\n",
    "    Trace gradient computation through a single training example.\n",
    "    Returns detailed breakdown of chain rule application.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    W, b = weights['W'], weights['b']\n",
    "    u = linear_forward(X_sample, W, b)\n",
    "    v = sigmoid_forward(u)\n",
    "    loss = mse_loss(np.array([[y_sample]]), v)\n",
    "    \n",
    "    # Backward pass with detailed tracking\n",
    "    dL_dv = mse_derivative(np.array([[y_sample]]), v)\n",
    "    dv_du = sigmoid_derivative(u)\n",
    "    dL_du = dL_dv * dv_du\n",
    "    dL_dW, dL_db, dL_dX = linear_backward(dL_du, X_sample, W)\n",
    "    \n",
    "    return {\n",
    "        'input': X_sample.flatten(),\n",
    "        'target': y_sample,\n",
    "        'weights': W.flatten(),\n",
    "        'bias': b[0],\n",
    "        'u': u[0, 0],\n",
    "        'v': v[0, 0],\n",
    "        'loss': loss,\n",
    "        'dL_dv': dL_dv[0, 0],\n",
    "        'dv_du': dv_du[0, 0],\n",
    "        'dL_du': dL_du[0, 0],\n",
    "        'dL_dW': dL_dW.flatten(),\n",
    "        'dL_db': dL_db[0],\n",
    "        'dL_dX': dL_dX.flatten()\n",
    "    }\n",
    "\n",
    "def analyze_training_curves(loss_histories, labels, title=\"Training Comparison\"):\n",
    "    \"\"\"\n",
    "    Plot and analyze multiple training curves.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for loss_history, label in zip(loss_histories, labels):\n",
    "        plt.plot(loss_history, label=label, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return final losses for comparison\n",
    "    return [history[-1] for history in loss_histories]\n",
    "\n",
    "def gradient_magnitude_analysis(X, y, weights, activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Analyze gradient magnitudes to understand training dynamics.\n",
    "    \"\"\"\n",
    "    y_formatted = y.reshape(-1, 1)\n",
    "    \n",
    "    if 'W1' in weights:  # Two layer\n",
    "        u1, v1 = single_layer_forward(X, weights['W1'], weights['b1'], activation)\n",
    "        u2, v2 = single_layer_forward(v1, weights['W2'], weights['b2'], 'sigmoid')\n",
    "        \n",
    "        dL_dv2 = mse_derivative(y_formatted, v2)\n",
    "        dL_dW2, dL_db2, dL_dv1 = single_layer_backward(dL_dv2, u2, v1, weights['W2'], 'sigmoid')\n",
    "        dL_dW1, dL_db1, dL_dX = single_layer_backward(dL_dv1, u1, X, weights['W1'], activation)\n",
    "        \n",
    "        return {\n",
    "            'layer1_weights': np.linalg.norm(dL_dW1),\n",
    "            'layer1_bias': np.linalg.norm(dL_db1),\n",
    "            'layer2_weights': np.linalg.norm(dL_dW2),\n",
    "            'layer2_bias': np.linalg.norm(dL_db2)\n",
    "        }\n",
    "    else:  # Single layer\n",
    "        u, v = single_layer_forward(X, weights['W'], weights['b'], activation)\n",
    "        dL_dv = mse_derivative(y_formatted, v)\n",
    "        dL_dW, dL_db, dL_dX = single_layer_backward(dL_dv, u, X, weights['W'], activation)\n",
    "        \n",
    "        return {\n",
    "            'weights': np.linalg.norm(dL_dW),\n",
    "            'bias': np.linalg.norm(dL_db)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Setup and Initial Analysis\n",
    "\n",
    "Generate the Swiss Roll dataset and create an initial visualization. \n",
    "\n",
    "**Your task**: Examine the data pattern and explain why this would be challenging for linear methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "X_swiss, y_swiss = generate_swiss_roll_2d(n_samples=300, noise=0.15)\n",
    "\n",
    "# Visualize the challenge\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_decision_boundary(X_swiss, y_swiss, predict_func=None, \n",
    "                      title=\"Swiss Roll Classification Challenge\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset shape: {X_swiss.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_swiss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Question 1**: Examine the data pattern above. Why would a single linear classifier (one neuron) struggle with this pattern? Write your explanation below:\n",
    "\n",
    "*[Your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Single Neuron Analysis\n",
    "\n",
    "Train a single neuron on the Swiss Roll data and analyze its performance.\n",
    "\n",
    "**Your task**: Train the network, visualize results, and analyze the chain rule for a specific example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train single neuron\n",
    "single_weights, single_loss, single_predict = train_simple_network(\n",
    "    X_swiss, y_swiss, hidden_size=0, epochs=200\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "plot_decision_boundary(X_swiss, y_swiss, single_predict, \n",
    "                      \"Single Neuron Decision Boundary\", ax=ax1)\n",
    "\n",
    "ax2.plot(single_loss, linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MSE Loss')\n",
    "ax2.set_title('Single Neuron Training Curve')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = single_predict(X_swiss)\n",
    "binary_preds = (predictions > 0.5).astype(int)\n",
    "accuracy = np.mean(binary_preds == y_swiss)\n",
    "print(f\"Single neuron accuracy: {accuracy:.3f}\")\n",
    "print(f\"Final loss: {single_loss[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule Demonstration\n",
    "\n",
    "**Your task**: Select a training example and trace the gradient computation step-by-step. Choose an example that helps illustrate the chain rule clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose an example index and justify your choice\n",
    "example_idx = None  # Replace with your chosen index\n",
    "\n",
    "# TODO: Explain why you chose this particular example\n",
    "print(\"Example selection rationale:\")\n",
    "print(\"[Your explanation here]\")\n",
    "\n",
    "# Demonstrate chain rule\n",
    "if example_idx is not None:\n",
    "    X_example = X_swiss[example_idx:example_idx+1]\n",
    "    y_example = y_swiss[example_idx]\n",
    "    \n",
    "    chain_rule_data = demonstrate_chain_rule_step_by_step(X_example, y_example, single_weights)\n",
    "    \n",
    "    # TODO: Display and interpret the chain rule computation\n",
    "    print(\"\\nChain Rule Step-by-Step:\")\n",
    "    print(f\"Input: {chain_rule_data['input']}\")\n",
    "    print(f\"Target: {chain_rule_data['target']}\")\n",
    "    # Add more detailed analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Question 2**: Based on your chain rule demonstration, explain how the gradient flows backward through the network. How does each partial derivative contribute to the final weight update?\n",
    "\n",
    "*[Your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Network Architecture Comparison\n",
    "\n",
    "Compare different network architectures on the same problem.\n",
    "\n",
    "**Your task**: Train networks with different complexities and analyze their performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design your architecture comparison experiment\n",
    "# Consider: hidden layer sizes, activation functions, training parameters\n",
    "\n",
    "architectures = [\n",
    "    # TODO: Define your architecture configurations\n",
    "    # Example format: {'hidden_size': 0, 'activation': 'sigmoid', 'label': 'Single Neuron'}\n",
    "]\n",
    "\n",
    "results = {}\n",
    "loss_histories = []\n",
    "labels = []\n",
    "\n",
    "# TODO: Train each architecture and collect results\n",
    "for config in architectures:\n",
    "    # Train network with this configuration\n",
    "    # Store results for comparison\n",
    "    pass\n",
    "\n",
    "# TODO: Create comparison visualizations\n",
    "# Consider: decision boundaries, training curves, accuracy comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Question 3**: Which architecture performed best and why? What trade-offs did you observe between model complexity and performance?\n",
    "\n",
    "*[Your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Activation Function Analysis\n",
    "\n",
    "Compare how different activation functions affect training dynamics.\n",
    "\n",
    "**Your task**: Train identical architectures with different activation functions and analyze the differences in gradient flow and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design activation function comparison\n",
    "activations = ['sigmoid', 'relu']  # Add others if you implement them\n",
    "\n",
    "activation_results = {}\n",
    "\n",
    "# TODO: Train networks with different activations\n",
    "# TODO: Analyze gradient magnitudes during training\n",
    "# TODO: Compare convergence behavior\n",
    "\n",
    "for activation in activations:\n",
    "    # Train and analyze\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Question 4**: How do different activation functions affect gradient flow? Which activation function worked best for this problem and why?\n",
    "\n",
    "*[Your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training Dynamics and Learning Rate Analysis\n",
    "\n",
    "Investigate how learning rate affects training dynamics.\n",
    "\n",
    "**Your task**: Test different learning rates and identify common training problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design learning rate experiment\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]  # Add more if needed\n",
    "\n",
    "lr_results = {}\n",
    "\n",
    "# TODO: Train with different learning rates\n",
    "# TODO: Identify and categorize training problems\n",
    "# TODO: Create diagnostic visualizations\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Train and diagnose\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Question 5**: What training problems did you observe with different learning rates? How would you diagnose these issues in practice?\n",
    "\n",
    "*[Your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Gradient Checking Validation\n",
    "\n",
    "Use the simple gradient checking function to validate your implementation.\n",
    "\n",
    "**Your task**: Perform gradient checking on your trained networks and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient checking analysis\n",
    "# Use the simple_gradient_check function from your student_code\n",
    "\n",
    "# Select a subset of data for checking\n",
    "X_check = X_swiss[:5]\n",
    "y_check = y_swiss[:5]\n",
    "\n",
    "# TODO: Define loss function for gradient checking\n",
    "# TODO: Get analytical gradients from your implementation\n",
    "# TODO: Use simple_gradient_check to validate\n",
    "# TODO: Interpret results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis Question 6**: What did gradient checking reveal about your implementation? How confident are you in your backpropagation code?\n",
    "\n",
    "*[Your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Executive Summary and Professional Insights\n",
    "\n",
    "**Your task**: Synthesize your findings into a professional analysis that could be shared with colleagues or stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "\n",
    "**Write a 2-3 paragraph summary addressing:**\n",
    "\n",
    "1. **Problem Complexity**: What made the Swiss Roll challenging for different approaches?\n",
    "2. **Architecture Insights**: How did network complexity affect solution quality?\n",
    "3. **Training Considerations**: What practical insights did you gain about neural network training?\n",
    "4. **Professional Applications**: How would you apply these insights to real-world projects?\n",
    "\n",
    "*[Your executive summary here]*\n",
    "\n",
    "### Key Technical Findings\n",
    "\n",
    "**List your 3-5 most important technical insights:**\n",
    "\n",
    "1. *[Finding 1]*\n",
    "2. *[Finding 2]*\n",
    "3. *[Finding 3]*\n",
    "4. *[Finding 4]*\n",
    "5. *[Finding 5]*\n",
    "\n",
    "### Recommendations for Practice\n",
    "\n",
    "**Based on your analysis, what practical recommendations would you make for neural network development?**\n",
    "\n",
    "*[Your recommendations here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peer Review Questions\n",
    "\n",
    "**For your peer reviewer:**\n",
    "\n",
    "1. **Analysis Depth**: Are the architecture comparisons thorough and well-justified? What additional experiments would strengthen the analysis?\n",
    "\n",
    "2. **Chain Rule Understanding**: Is the gradient flow explanation clear and mathematically sound? How could it be improved?\n",
    "\n",
    "3. **Professional Insights**: Do the findings translate to practical guidance for neural network development? Are the recommendations actionable?\n",
    "\n",
    "4. **Experimental Design**: Are the experimental choices (learning rates, architectures, etc.) well-motivated? What would you change?\n",
    "\n",
    "5. **Communication**: Is the technical content accessible to someone learning neural networks? Where could explanations be clearer?\n",
    "\n",
    "**Reviewer Guidelines:**\n",
    "- Focus on the quality of analysis and interpretation, not just correctness\n",
    "- Consider whether the insights would help someone understand neural networks better\n",
    "- Suggest specific improvements for unclear sections\n",
    "- Evaluate the professional relevance of the findings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}